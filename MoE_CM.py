#0317-æ”¹è¿›çš„æœ€ç»ˆä»£ç ï¼Œå¯ä»¥æ›¿æ¢æ··æ·†çŸ©é˜µè¿›è¡Œå®éªŒ
# èåˆåAccuracy: 0.9341
# èåˆç»“æœ Hamming_loss:0.0659, MAP:0.6826, AUC: 0.8852, F1: 0.6617, Precision: 0.8233, Recall: 0.6102


import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import hamming_loss, roc_auc_score, f1_score, precision_score, recall_score
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from PIL import Image
import os
from torchvision.models import ResNet50_Weights
from torch.optim.lr_scheduler import ReduceLROnPlateau
import random
import torch.nn.functional as F



# æå–å›¾åƒç‰¹å¾å¹¶ä¿å­˜åˆ°æ–‡ä»¶
def extract_and_save_features(data, image_folder, transform, output_path):
    model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
    model = nn.Sequential(*list(model.children())[:-1])  # å»æ‰æœ€åçš„åˆ†ç±»å±‚
    model.eval()

    features_dict = {}
    with torch.no_grad():
        for idx, row in data.iterrows():
            image_path = os.path.join(image_folder, row['Image_ID'])
            print(f"æå–ç‰¹å¾: {image_path}")
            try:
                image = Image.open(image_path).convert('RGB')
                image = transform(image).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
                features = model(image).squeeze().numpy()
                features_dict[row['Image_ID']] = features
            except Exception as e:
                print(f"è·³è¿‡å›¾åƒ {image_path}: {e}")

    # ä¿å­˜æå–çš„ç‰¹å¾
    np.save(output_path, features_dict)
    print(f"ç‰¹å¾å·²ä¿å­˜åˆ° {output_path}")


# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class FusionDataset(Dataset):
    def __init__(self, data, feature_file):
        self.data = data.fillna(0)  # å¡«å……æ•°æ®é›†ä¸­çš„ç©ºå€¼ä¸º0
        self.features = np.load(feature_file, allow_pickle=True).item()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image_id = row['Image_ID']

        # åŠ è½½å›¾åƒç‰¹å¾
        image_features = self.features.get(image_id, np.zeros(2048))

        expert_labels = row[['Fracture_Expert_Label', 'Pneumothorax_Expert_Label',
                             'Airspace_Opacity_Expert_Label', 'Nodule_Or_Mass_Expert_Label']].values.astype(float)
        ml_predictions = row[['Fracture_ML_Label', 'Pneumothorax_ML_Label',
                              'Airspace_Opacity_ML_Label', 'Nodule_Or_Mass_ML_Label']].values.astype(float)
        true_labels = row[['Fracture_GT_Label', 'Pneumothorax_GT_Label',
                           'Airspace_Opacity_GT_Label', 'Nodule_Or_Mass_GT_Label']].values.astype(float)
        

        # print(f"image_features.shape: {image_features.shape}")
        # print(f"expert_labels.shape: {expert_labels.shape}")
        # print(f"ml_predictions.shape: {ml_predictions.shape}")

        return {
            'image_features': torch.tensor(image_features, dtype=torch.float32),
            'expert_labels': torch.tensor(expert_labels, dtype=torch.float32),
            'ml_predictions': torch.tensor(ml_predictions, dtype=torch.float32),
            'true_labels': torch.tensor(true_labels, dtype=torch.float32),
            'Image_ID': image_id
        }

# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class FusionDataset(Dataset):
    def __init__(self, data, feature_file):
        self.data = data.fillna(0)  # å¡«å……æ•°æ®é›†ä¸­çš„ç©ºå€¼ä¸º0
        self.features = np.load(feature_file, allow_pickle=True).item()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image_id = row['Image_ID']

        # åŠ è½½å›¾åƒç‰¹å¾
        image_features = self.features.get(image_id, np.zeros(2048))

        expert_labels = row[['Fracture_Expert_Label', 'Pneumothorax_Expert_Label',
                             'Airspace_Opacity_Expert_Label', 'Nodule_Or_Mass_Expert_Label']].values.astype(float)
 
        ml_predictions = row[['Fracture_ML_Label_alexnet', 'Pneumothorax_ML_Label_alexnet',
                        'Airspace_Opacity_ML_Label_alexnet', 'Nodule_Or_Mass_ML_Label_alexnet']].values.astype(float)  
        
        true_labels = row[['Fracture_GT_Label', 'Pneumothorax_GT_Label',
                           'Airspace_Opacity_GT_Label', 'Nodule_Or_Mass_GT_Label']].values.astype(float)
        
        # print(f"image_features.shape: {image_features.shape}")
        # print(f"expert_labels.shape: {expert_labels.shape}")
        # print(f"ml_predictions.shape: {ml_predictions.shape}")

        return {
            'image_features': torch.tensor(image_features, dtype=torch.float32),
            'expert_labels': torch.tensor(expert_labels, dtype=torch.float32),
            'ml_predictions': torch.tensor(ml_predictions, dtype=torch.float32),
            'true_labels': torch.tensor(true_labels, dtype=torch.float32),
            'Image_ID': image_id
        }


class TransformerEncoder(nn.Module):
    def __init__(self, input_dim, num_heads=2, num_layers=2, hidden_dim=128):
        super(TransformerEncoder, self).__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        return self.transformer(x)

class Gating(nn.Module):
    def __init__(self, input_dim, num_experts, num_labels, dropout_rate=0.4, confusion_matrix=None):
        super(Gating, self).__init__()
        self.num_labels = num_labels
        self.num_experts = num_experts
        # è¾“å…¥ç»´åº¦è°ƒæ•´ä¸º image_features + expert_labels + ml_predictions
        total_input_dim = input_dim + 2 * num_labels  
        self.gates = nn.ModuleList([self._create_gate(total_input_dim, num_experts, dropout_rate) for _ in range(num_labels)])
        
        if confusion_matrix is not None:
            self.register_buffer("confusion_matrix", torch.tensor(confusion_matrix, dtype=torch.float32))  # (num_labels, num_labels)
            self.transformer = TransformerEncoder(input_dim=num_labels)  # Transformer å­¦ä¹ æ··æ·†çŸ©é˜µæ¨¡å¼
            self.confusion_proj = nn.Linear(num_labels, num_experts)  # è®© transformer è¾“å‡ºçš„ä¿¡æ¯å½±å“ gating

    def _create_gate(self, input_dim, num_experts, dropout_rate):
        gate = nn.Sequential(
            nn.Linear(input_dim, 256),  # å¢åŠ ç¬¬ä¸€å±‚å®½åº¦
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.LayerNorm(128),         # æ”¹ç”¨LayerNorm
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate/2),# é™ä½dropoutç‡
            nn.Linear(128, num_experts),
            nn.Softmax(dim=1)
        )
        # æ·»åŠ å‚æ•°åˆå§‹åŒ–
        for layer in gate:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='leaky_relu')
                nn.init.constant_(layer.bias, 0.1)
        return gate

    def forward(self, x):
        """
        x: (batch_size, input_dim)
        è¿”å›: (batch_size, num_labels, num_experts)
        """
        gating_weights = [gate(x) for gate in self.gates]  # (batch_size, num_labels, num_experts)
        gating_weights = torch.stack(gating_weights, dim=1)  # (batch_size, num_labels, num_experts)

        # å¤„ç†æ··æ·†çŸ©é˜µä¿¡æ¯
        if hasattr(self, "confusion_matrix"):
            # é€šè¿‡ Transformer å­¦ä¹ æ··æ·†çŸ©é˜µçš„ç‰¹å¾
            learned_confusion = self.transformer(self.confusion_matrix.unsqueeze(0)).squeeze(0)  # (num_labels, num_labels)
            
            # æŠ•å½±åˆ° gating ç»´åº¦ï¼Œä½¿ gating è®¡ç®—æ—¶å—å½±å“
            confusion_gating_factors = self.confusion_proj(learned_confusion)  # (num_labels, num_experts)

            # è®©æ‰€æœ‰ batch å…±äº«ç›¸åŒçš„ confusion_gating_factors
            confusion_gating_factors = confusion_gating_factors.unsqueeze(0).expand(x.shape[0], -1, -1)  # (batch_size, num_labels, num_experts)

            # é€šè¿‡ softmax é‡æ–°è®¡ç®— gating æƒé‡ï¼Œä½¿ gating å—æ··æ·†ä¿¡æ¯å½±å“
            gating_weights = torch.softmax(gating_weights * torch.sigmoid(confusion_gating_factors), dim=2)

        return gating_weights

class MoE(nn.Module):
    def __init__(self, input_dim, num_experts=2, num_labels=4, confusion_matrix=None):
        super(MoE, self).__init__()
        self.gating = Gating(input_dim=input_dim, num_experts=num_experts, num_labels=num_labels, confusion_matrix=confusion_matrix)
    
    def forward(self, image_features, expert_labels,ml_predictions):
        # æ‹¼æ¥ä¸‰è€…ä½œä¸ºé—¨æ§è¾“å…¥
        gate_input = torch.cat([image_features, expert_labels, ml_predictions], dim=1)
        gating_weights = self.gating(gate_input)  # åªä¼ é€’æ‹¼æ¥åçš„è¾“å…¥
        
        # 1. è®© Transformer å­¦åˆ°çš„æ··æ·†ä¿¡æ¯ä½œç”¨äºä¸“å®¶è¾“å‡º
        confusion_matrix = self.gating.confusion_matrix  # (num_labels, num_labels)
        learned_confusion = self.gating.transformer(confusion_matrix.unsqueeze(0)).squeeze(0)  # (num_labels, num_labels)

        # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä½¿ human_expert å—å½±å“
        attention_weights = torch.softmax(learned_confusion, dim=1)  # (num_labels, num_labels)

        # 3. é‡æ–°è°ƒæ•´ expert_labelsï¼Œä½¿å…¶å—åˆ° attention_weights çš„å½±å“
        adjusted_expert_labels = torch.matmul(expert_labels, attention_weights)  # (batch_size, num_labels)

        # 4. ç»„åˆä¸“å®¶è¾“å‡º (ç¬¬ä¸€ä¸ªæ˜¯äººç±»ä¸“å®¶, ç¬¬äºŒä¸ªæ˜¯æœºå™¨ä¸“å®¶)
        expert_outputs = torch.stack([adjusted_expert_labels, ml_predictions], dim=2)  # (batch_size, num_labels, num_experts)

        # 5. è®¡ç®—åŠ æƒå’Œ
        fused_output = torch.sum(expert_outputs * gating_weights, dim=2)  # (batch_size, num_labels)

        return torch.sigmoid(fused_output)



    
# å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
def find_best_threshold(predictions, true_labels):
    best_thresholds = []
    for i in range(predictions.shape[1]):  # å‡è®¾æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç±»åˆ«
        thresholds = np.linspace(0, 1, 100)
        losses = [hamming_loss(true_labels[:, i], (predictions[:, i] > t).astype(int)) for t in thresholds]
        best_threshold = thresholds[np.argmin(losses)]
        best_thresholds.append(best_threshold)
    return best_thresholds


def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, model_save_path="best_model.pth"):
    best_val_loss = float('inf')  # åˆå§‹å€¼ä¸ºæ­£æ— ç©·
    best_epoch = -1
    # best_thresholds = None  # ç”¨äºå­˜å‚¨æœ€ä½³é˜ˆå€¼
    best_thresholds = 0.5
    early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)  # è¿™é‡Œé€‰æ‹©æŒ‰éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡

    # è®°å½•æ¯ä¸ª epoch çš„ Hamming Loss å’Œ AUC
    train_losses = []
    val_losses = []
    val_hamming_losses = []
    val_aucs = []

    for epoch in range(epochs):
        print(f"å¼€å§‹è®­ç»ƒ Epoch {epoch + 1}/{epochs}")
        model.train()
        total_train_loss = 0
        
        # è®­ç»ƒé˜¶æ®µ
        for batch in train_loader:
            image_features = batch['image_features']
            expert_labels = batch['expert_labels']
            ml_predictions = batch['ml_predictions']
            true_labels = batch['true_labels']
            
            optimizer.zero_grad()
            outputs = model(image_features, expert_labels, ml_predictions)
            loss = criterion(outputs, true_labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            total_train_loss += loss.item()
        
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        all_val_outputs = []
        all_val_labels = []
        all_gating_weights = []  # ç”¨äºå­˜å‚¨æƒé‡
        
        with torch.no_grad():
            for batch in val_loader:
                image_features = batch['image_features']
                expert_labels = batch['expert_labels']
                ml_predictions = batch['ml_predictions']
                true_labels = batch['true_labels']

                outputs = model(image_features, expert_labels, ml_predictions)
                loss = criterion(outputs, true_labels)
                total_val_loss += loss.item()
                all_val_outputs.append(outputs)
                all_val_labels.append(true_labels)
                gate_input = torch.cat([image_features, expert_labels, ml_predictions], dim=1)
                all_gating_weights.append(model.gating(gate_input))  # è·å–æƒé‡

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Val Loss: {avg_val_loss:.4f}")

        # è®¡ç®— Hamming Loss å’Œ AUC
        all_val_outputs = torch.cat(all_val_outputs).numpy()
        all_val_labels = torch.cat(all_val_labels).numpy()
        val_hamming_loss = hamming_loss(all_val_labels, (all_val_outputs >= 0.5).astype(int))
        val_auc = roc_auc_score(all_val_labels, all_val_outputs, average='macro')
        val_hamming_losses.append(val_hamming_loss)
        val_aucs.append(val_auc)
        print(f"Epoch {epoch + 1}/{epochs}, Val Hamming Loss: {val_hamming_loss:.4f}, Val AUC: {val_auc:.4f}")

        # è¾“å‡ºæƒé‡
        all_gating_weights = torch.cat(all_gating_weights).numpy()
        print(f"Epoch {epoch + 1}/{epochs}, Gating Weights: {all_gating_weights}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            early_stop_counter = 0  # é‡æ–°è®¡æ•°
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_val_loss,
            }, model_save_path)
            print(f"æ–°æœ€ä½³æ¨¡å‹ä¿å­˜äº Epoch {epoch + 1}, Val Loss: {avg_val_loss:.4f}")

            # è®¡ç®—æœ€ä½³é˜ˆå€¼
            # best_thresholds = find_best_threshold(all_val_outputs, all_val_labels)
        else:
            early_stop_counter += 1

        if early_stop_counter >= 3:
            print(f"ğŸ”¥ è®­ç»ƒæå‰åœæ­¢äº Epoch {epoch+1}ï¼Œå› ä¸ºéªŒè¯æŸå¤± 3 è½®æ²¡æœ‰ä¸‹é™")
            break
        # æ›´æ–°å­¦ä¹ ç‡
        # scheduler.step(avg_val_loss)  # æ¯ä¸ª epoch ç»“æŸåæ ¹æ®éªŒè¯æŸå¤±æ›´æ–°å­¦ä¹ ç‡

    print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º {best_val_loss:.4f}")
    
    return best_thresholds

# # æµ‹è¯•æ¨¡å‹
from sklearn.metrics import average_precision_score

def calculate_map(predictions, true_labels):
    """
    è®¡ç®— Mean Average Precision (MAP)ã€‚
    
    :param predictions: æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡å€¼ï¼Œshape (batch_size, num_labels)ï¼ŒNumPy æ•°ç»„æ ¼å¼
    :param true_labels: çœŸå®æ ‡ç­¾ï¼Œshape (batch_size, num_labels)ï¼ŒNumPy æ•°ç»„æ ¼å¼
    :return: MAP å€¼
    """
    num_labels = true_labels.shape[1]  # ä½¿ç”¨ NumPy æ•°ç»„çš„ shape å±æ€§
    aps = []

    for i in range(num_labels):
        ap = average_precision_score(true_labels[:, i], predictions[:, i])
        aps.append(ap)

    map_value = np.mean(aps)  # è®¡ç®— MAP
    return map_value
from sklearn.metrics import roc_auc_score

def evaluate_model(model, loader, thresholds, model_path='best_model.pth', output_csv='fusion_results.csv'):
    # åŠ è½½ä¿å­˜çš„æœ€ä¼˜æ¨¡å‹
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    print(f"å·²åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒEpoch {checkpoint['epoch']}ï¼ŒéªŒè¯æŸå¤±ä¸º {checkpoint['loss']:.4f}")

    all_outputs = []
    all_labels = []
    all_image_ids = []
    all_expert_labels = []
    all_ml_predictions = []
    all_gating_weights = []  # ç”¨äºå­˜å‚¨æƒé‡

    with torch.no_grad():
        for batch in loader:
            image_features = batch['image_features']
            expert_labels = batch['expert_labels']
            ml_predictions = batch['ml_predictions']
            true_labels = batch['true_labels']
            image_ids = batch['Image_ID']

            outputs = model(image_features, expert_labels, ml_predictions)
            all_outputs.append(outputs)
            all_labels.append(true_labels)
            all_image_ids.extend(image_ids)
            all_expert_labels.append(expert_labels)
            all_ml_predictions.append(ml_predictions)
            gate_input = torch.cat([image_features, expert_labels, ml_predictions], dim=1)
            all_gating_weights.append(model.gating(gate_input))  # è·å–æƒé‡

    all_outputs = torch.cat(all_outputs).numpy()
    all_labels = torch.cat(all_labels).numpy()
    all_expert_labels = torch.cat(all_expert_labels).numpy()
    all_ml_predictions = torch.cat(all_ml_predictions).numpy()
    all_gating_weights = torch.cat(all_gating_weights).numpy()

    # åº”ç”¨é˜ˆå€¼
    predictions = (all_outputs >= np.array(thresholds)).astype(int)
    fusion_hamming_loss = hamming_loss(all_labels, predictions)
    expert_hamming_loss = hamming_loss(all_labels, (all_expert_labels >= 0.5).astype(int))
    ml_hamming_loss = hamming_loss(all_labels, (all_ml_predictions >= 0.5).astype(int))



    # print(f"èåˆåHamming Loss: {fusion_hamming_loss:.4f}")

    # è®¡ç®—Accuracy
    accuracy_fusion = (predictions == all_labels).mean()
    print(f"èåˆåAccuracy: {accuracy_fusion:.4f}")

    map_fusion = calculate_map(all_outputs, all_labels)
    map_expert = calculate_map(all_expert_labels, all_labels)
    map_ml = calculate_map(all_ml_predictions, all_labels)


    # è®¡ç®—AUC
    auc_fusion = roc_auc_score(all_labels, all_outputs, average='macro')
    auc_expert = roc_auc_score(all_labels, all_expert_labels, average='macro')
    auc_ml = roc_auc_score(all_labels, all_ml_predictions, average='macro')

    # è¾“å‡ºæƒé‡
    # print(f"Gating Weights: {all_gating_weights}")

    f1_fusion = f1_score(all_labels, predictions, average='macro')
    f1_expert = f1_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    f1_ml = f1_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')

    precision_fusion = precision_score(all_labels, predictions, average='macro')
    precision_expert = precision_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    precision_ml = precision_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')

    recall_fusion = recall_score(all_labels, predictions, average='macro')
    recall_expert = recall_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    recall_ml = recall_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')


    print(f"èåˆç»“æœ Hamming_loss:{fusion_hamming_loss:.4f}, MAP:{map_fusion:.4f}, AUC: {auc_fusion:.4f}, F1: {f1_fusion:.4f}, Precision: {precision_fusion:.4f}, Recall: {recall_fusion:.4f}")
    print(f"äººç±»ä¸“å®¶ Hamming_loss:{expert_hamming_loss:.4f}, MAP:{map_expert:.4f}, AUC: {auc_expert:.4f}, F1: {f1_expert:.4f}, Precision: {precision_expert:.4f}, Recall: {recall_expert:.4f}")
    print(f"æœºå™¨ä¸“å®¶ Hamming_loss:{ml_hamming_loss:.4f}, MAP:{map_ml:.4f}, AUC: {auc_ml:.4f}, F1: {f1_ml:.4f}, Precision: {precision_ml:.4f}, Recall: {recall_ml:.4f}")

    # print(f"Gating Weights: {all_gating_weights}")
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„AUC
    auc_fusion_per_label = []
    for i, label in enumerate(['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass']):
        try:
            auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])
            auc_fusion_per_label.append(auc)
            print(f"{label} AUC: {auc:.4f}")
        except ValueError:
            auc_fusion_per_label.append(np.nan)
            print(f"{label} AUC: æ— æ³•è®¡ç®—ï¼ˆå¯èƒ½æ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»åˆ«ï¼‰")

    results_df = pd.DataFrame(all_outputs, columns=['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass'])
    results_df['Image_ID'] = all_image_ids
    results_df.to_csv(output_csv, index=False)
    print(f"ç»“æœå·²ä¿å­˜åˆ° {output_csv}")

    log_file='test0420_evaluation_wo_thr.txt'
    with open(log_file, 'a') as f:
        f.write(f"èåˆåHamming Loss: {fusion_hamming_loss:.4f}\n")
        f.write(f"èåˆç»“æœ MAP:{map_fusion:.4f}, AUC: {auc_fusion:.4f}, F1: {f1_fusion:.4f}, Precision: {precision_fusion:.4f}, Recall: {recall_fusion:.4f}\n")
        f.write(f"äººç±»ä¸“å®¶ MAP:{map_expert:.4f}, AUC: {auc_expert:.4f}, F1: {f1_expert:.4f}, Precision: {precision_expert:.4f}, Recall: {recall_expert:.4f}\n")
        f.write(f"æœºå™¨ä¸“å®¶ MAP:{map_ml:.4f}, AUC: {auc_ml:.4f}, F1: {f1_ml:.4f}, Precision: {precision_ml:.4f}, Recall: {recall_ml:.4f}\n")

# å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

confusion_matrix = np.array([
    [0.42, 0.01, 0.23, 0.02],
    [0.01, 0.71, 0.19, 0.02],
    [0.00, 0.01, 0.83, 0.03],
    [0.01, 0.01, 0.17, 0.62]
])



# è®¾ç½®è·¯å¾„
image_folder = '/data/user24262904/My/ChestX-ray14/images'
data_path = "all_predictions.csv"

data = pd.read_csv(data_path)


# æå–å¹¶ä¿å­˜ç‰¹å¾
feature_file = 'image_features.npy'
if not os.path.exists(feature_file):
    extract_and_save_features(data, image_folder, transform, feature_file)


# æ•°æ®åˆ†å‰²
train_data = data[data['Set_ID_x'] == 'train']
val_data = data[data['Set_ID_x'] == 'val']
test_data = data[data['Set_ID_x'] == 'test']

train_dataset = FusionDataset(train_data, feature_file)
val_dataset = FusionDataset(val_data, feature_file)
test_dataset = FusionDataset(test_data, feature_file)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# åˆå§‹åŒ–æ¨¡å‹
# åˆå§‹åŒ–MoEæ¨¡å‹
print("è®­ç»ƒç¬¬1æ¬¡-----------------------------------")
# è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# è®­ç»ƒå’Œæµ‹è¯•
thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')

# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬2æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')

# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬3æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬4æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬5æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬6æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬7æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬8æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬9æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬10æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='test_results1.csv')
