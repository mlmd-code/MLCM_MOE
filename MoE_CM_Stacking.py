#0408-å°†é—¨æ§ç½‘ç»œæ›¿æ¢ä¸ºå †å æ–¹æ³•
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import hamming_loss, roc_auc_score, f1_score, precision_score, recall_score
from torch.utils.data import DataLoader, Dataset
from torchvision import models, transforms
from PIL import Image
import os
from torchvision.models import ResNet50_Weights
from torch.optim.lr_scheduler import ReduceLROnPlateau
import random
import torch.nn.functional as F



# æå–å›¾åƒç‰¹å¾å¹¶ä¿å­˜åˆ°æ–‡ä»¶
def extract_and_save_features(data, image_folder, transform, output_path):
    model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
    model = nn.Sequential(*list(model.children())[:-1])  # å»æ‰æœ€åçš„åˆ†ç±»å±‚
    model.eval()

    features_dict = {}
    with torch.no_grad():
        for idx, row in data.iterrows():
            image_path = os.path.join(image_folder, row['Image_ID'])
            print(f"æå–ç‰¹å¾: {image_path}")
            try:
                image = Image.open(image_path).convert('RGB')
                image = transform(image).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
                features = model(image).squeeze().numpy()
                features_dict[row['Image_ID']] = features
            except Exception as e:
                print(f"è·³è¿‡å›¾åƒ {image_path}: {e}")

    # ä¿å­˜æå–çš„ç‰¹å¾
    np.save(output_path, features_dict)
    print(f"ç‰¹å¾å·²ä¿å­˜åˆ° {output_path}")


# è‡ªå®šä¹‰æ•°æ®é›†ç±»
class FusionDataset(Dataset):
    def __init__(self, data, feature_file):
        self.data = data.fillna(0)  # å¡«å……æ•°æ®é›†ä¸­çš„ç©ºå€¼ä¸º0
        self.features = np.load(feature_file, allow_pickle=True).item()

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image_id = row['Image_ID']

        # åŠ è½½å›¾åƒç‰¹å¾
        image_features = self.features.get(image_id, np.zeros(2048))

        expert_labels = row[['Fracture_Expert_Label', 'Pneumothorax_Expert_Label',
                             'Airspace_Opacity_Expert_Label', 'Nodule_Or_Mass_Expert_Label']].values.astype(float)
        ml_predictions = row[['Fracture_ML_Label', 'Pneumothorax_ML_Label',
                              'Airspace_Opacity_ML_Label', 'Nodule_Or_Mass_ML_Label']].values.astype(float)
        true_labels = row[['Fracture_GT_Label', 'Pneumothorax_GT_Label',
                           'Airspace_Opacity_GT_Label', 'Nodule_Or_Mass_GT_Label']].values.astype(float)
        

        # print(f"image_features.shape: {image_features.shape}")
        # print(f"expert_labels.shape: {expert_labels.shape}")
        # print(f"ml_predictions.shape: {ml_predictions.shape}")

        return {
            'image_features': torch.tensor(image_features, dtype=torch.float32),
            'expert_labels': torch.tensor(expert_labels, dtype=torch.float32),
            'ml_predictions': torch.tensor(ml_predictions, dtype=torch.float32),
            'true_labels': torch.tensor(true_labels, dtype=torch.float32),
            'Image_ID': image_id
        }



class TransformerEncoder(nn.Module):
    def __init__(self, input_dim, num_heads=2, num_layers=2, hidden_dim=128):
        super(TransformerEncoder, self).__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        return self.transformer(x)
    
class StackingGating(nn.Module):
    def __init__(self, input_dim, num_experts, num_labels, dropout_rate=0.4, confusion_matrix=None):
        super(StackingGating, self).__init__()
        self.num_labels = num_labels
        self.num_experts = num_experts
        
        # ä»…ä½¿ç”¨å›¾åƒç‰¹å¾çš„å †å æ¨¡å‹ (è¾“å…¥ç»´åº¦ä¸åŸå§‹Gatingä¸€è‡´)
        self.meta_model = nn.Sequential(
            nn.Linear(input_dim, 128),  # ä»…2048ç»´å›¾åƒç‰¹å¾
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, num_labels * num_experts),
            nn.Unflatten(1, (num_labels, num_experts)),
            nn.Softmax(dim=2)
        )

        # ä¿æŒåŸå§‹æ··æ·†çŸ©é˜µå¤„ç†
        if confusion_matrix is not None:
            self.register_buffer("confusion_matrix", torch.tensor(confusion_matrix, dtype=torch.float32))
            self.transformer = TransformerEncoder(input_dim=num_labels)
            self.confusion_proj = nn.Linear(num_labels, num_experts)

    def forward(self, image_features, expert_labels=None, ml_predictions=None):  # ä¿æŒæ¥å£å…¼å®¹
        # ä»…ä½¿ç”¨å›¾åƒç‰¹å¾
        gating_weights = self.meta_model(image_features)  # (batch,4,2)
        
        # ä¿æŒåŸå§‹æ··æ·†çŸ©é˜µå¤„ç†é€»è¾‘
        if hasattr(self, "confusion_matrix"):
            learned_confusion = self.transformer(self.confusion_matrix.unsqueeze(0)).squeeze(0)
            confusion_gating_factors = self.confusion_proj(learned_confusion)
            confusion_gating_factors = confusion_gating_factors.unsqueeze(0).expand(image_features.shape[0], -1, -1)
            gating_weights = torch.softmax(gating_weights * torch.sigmoid(confusion_gating_factors), dim=2)
        
        return gating_weights


class MoE(nn.Module):
    def __init__(self, input_dim, num_experts=2, num_labels=4, confusion_matrix=None):
        super(MoE, self).__init__()
        # ä½¿ç”¨StackingGatingæ›¿ä»£åŸæ¥çš„Gating
        self.gating = StackingGating(
            input_dim=input_dim,
            num_experts=num_experts,
            num_labels=num_labels,
            confusion_matrix=confusion_matrix
        )

    def forward(self, image_features, expert_labels, ml_predictions):
        """
        ä¿æŒä¸åŸå§‹MoEå®Œå…¨ç›¸åŒçš„forwardé€»è¾‘
        """
        gating_weights = self.gating(image_features, expert_labels, ml_predictions)  # (batch,4,2)

        # 1. è®©Transformerå­¦åˆ°çš„æ··æ·†ä¿¡æ¯ä½œç”¨äºä¸“å®¶è¾“å‡º
        confusion_matrix = self.gating.confusion_matrix  # (4,4)
        learned_confusion = self.gating.transformer(confusion_matrix.unsqueeze(0)).squeeze(0)  # (4,4)

        # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä½¿human_expertå—å½±å“
        attention_weights = torch.softmax(learned_confusion, dim=1)  # (4,4)

        # 3. é‡æ–°è°ƒæ•´expert_labels
        adjusted_expert_labels = torch.matmul(expert_labels, attention_weights)  # (batch,4)

        # 4. ç»„åˆä¸“å®¶è¾“å‡º
        expert_outputs = torch.stack([adjusted_expert_labels, ml_predictions], dim=2)  # (batch,4,2)

        # 5. è®¡ç®—åŠ æƒå’Œ
        fused_output = torch.sum(expert_outputs * gating_weights, dim=2)  # (batch,4)

        return torch.sigmoid(fused_output)


    
# å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
def find_best_threshold(predictions, true_labels):
    best_thresholds = []
    for i in range(predictions.shape[1]):  # å‡è®¾æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç±»åˆ«
        thresholds = np.linspace(0, 1, 100)
        losses = [hamming_loss(true_labels[:, i], (predictions[:, i] > t).astype(int)) for t in thresholds]
        best_threshold = thresholds[np.argmin(losses)]
        best_thresholds.append(best_threshold)
    return best_thresholds


def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, model_save_path="best_model.pth"):
    best_val_loss = float('inf')  # åˆå§‹å€¼ä¸ºæ­£æ— ç©·
    best_epoch = -1
    best_thresholds = None  # ç”¨äºå­˜å‚¨æœ€ä½³é˜ˆå€¼
    early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    # scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)  # è¿™é‡Œé€‰æ‹©æŒ‰éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡

    # è®°å½•æ¯ä¸ª epoch çš„ Hamming Loss å’Œ AUC
    train_losses = []
    val_losses = []
    val_hamming_losses = []
    val_aucs = []

    for epoch in range(epochs):
        print(f"å¼€å§‹è®­ç»ƒ Epoch {epoch + 1}/{epochs}")
        model.train()
        total_train_loss = 0
        
        # è®­ç»ƒé˜¶æ®µ
        for batch in train_loader:
            image_features = batch['image_features']
            expert_labels = batch['expert_labels']
            ml_predictions = batch['ml_predictions']
            true_labels = batch['true_labels']
            
            optimizer.zero_grad()
            outputs = model(image_features, expert_labels, ml_predictions)
            loss = criterion(outputs, true_labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            total_train_loss += loss.item()
        
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        total_val_loss = 0
        all_val_outputs = []
        all_val_labels = []
        all_gating_weights = []  # ç”¨äºå­˜å‚¨æƒé‡
        
        with torch.no_grad():
            for batch in val_loader:
                image_features = batch['image_features']
                expert_labels = batch['expert_labels']
                ml_predictions = batch['ml_predictions']
                true_labels = batch['true_labels']

                outputs = model(image_features, expert_labels, ml_predictions)
                loss = criterion(outputs, true_labels)
                total_val_loss += loss.item()
                all_val_outputs.append(outputs)
                all_val_labels.append(true_labels)
                all_gating_weights.append(model.gating(image_features, expert_labels, ml_predictions))  # è·å–æƒé‡

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Val Loss: {avg_val_loss:.4f}")

        # è®¡ç®— Hamming Loss å’Œ AUC
        all_val_outputs = torch.cat(all_val_outputs).numpy()
        all_val_labels = torch.cat(all_val_labels).numpy()
        val_hamming_loss = hamming_loss(all_val_labels, (all_val_outputs >= 0.5).astype(int))
        val_auc = roc_auc_score(all_val_labels, all_val_outputs, average='macro')
        val_hamming_losses.append(val_hamming_loss)
        val_aucs.append(val_auc)
        print(f"Epoch {epoch + 1}/{epochs}, Val Hamming Loss: {val_hamming_loss:.4f}, Val AUC: {val_auc:.4f}")

        # è¾“å‡ºæƒé‡
        all_gating_weights = torch.cat(all_gating_weights).numpy()
        # print(f"Epoch {epoch + 1}/{epochs}, Gating Weights: {all_gating_weights}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯å½“å‰æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            early_stop_counter = 0  # é‡æ–°è®¡æ•°
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_val_loss,
            }, model_save_path)
            print(f"æ–°æœ€ä½³æ¨¡å‹ä¿å­˜äº Epoch {epoch + 1}, Val Loss: {avg_val_loss:.4f}")

            # è®¡ç®—æœ€ä½³é˜ˆå€¼
            best_thresholds = find_best_threshold(all_val_outputs, all_val_labels)
        else:
            early_stop_counter += 1

        if early_stop_counter >= 3:
            print(f"ğŸ”¥ è®­ç»ƒæå‰åœæ­¢äº Epoch {epoch+1}ï¼Œå› ä¸ºéªŒè¯æŸå¤± 3 è½®æ²¡æœ‰ä¸‹é™")
            break
        # æ›´æ–°å­¦ä¹ ç‡
        # scheduler.step(avg_val_loss)  # æ¯ä¸ª epoch ç»“æŸåæ ¹æ®éªŒè¯æŸå¤±æ›´æ–°å­¦ä¹ ç‡

    print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹åœ¨ Epoch {best_epoch}ï¼ŒéªŒè¯æŸå¤±ä¸º {best_val_loss:.4f}")
    
    return best_thresholds  


# # æµ‹è¯•æ¨¡å‹
from sklearn.metrics import average_precision_score

def calculate_map(predictions, true_labels):
    """
    è®¡ç®— Mean Average Precision (MAP)ã€‚
    
    :param predictions: æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡å€¼ï¼Œshape (batch_size, num_labels)ï¼ŒNumPy æ•°ç»„æ ¼å¼
    :param true_labels: çœŸå®æ ‡ç­¾ï¼Œshape (batch_size, num_labels)ï¼ŒNumPy æ•°ç»„æ ¼å¼
    :return: MAP å€¼
    """
    num_labels = true_labels.shape[1]  # ä½¿ç”¨ NumPy æ•°ç»„çš„ shape å±æ€§
    aps = []

    for i in range(num_labels):
        ap = average_precision_score(true_labels[:, i], predictions[:, i])
        aps.append(ap)

    map_value = np.mean(aps)  # è®¡ç®— MAP
    return map_value
from sklearn.metrics import roc_auc_score

def evaluate_model(model, loader, thresholds, model_path='best_model.pth', output_csv='fusion_results.csv'):
    # åŠ è½½ä¿å­˜çš„æœ€ä¼˜æ¨¡å‹
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    print(f"å·²åŠ è½½æœ€ä½³æ¨¡å‹ï¼ŒEpoch {checkpoint['epoch']}ï¼ŒéªŒè¯æŸå¤±ä¸º {checkpoint['loss']:.4f}")

    all_outputs = []
    all_labels = []
    all_image_ids = []
    all_expert_labels = []
    all_ml_predictions = []
    all_gating_weights = []  # ç”¨äºå­˜å‚¨æƒé‡

    with torch.no_grad():
        for batch in loader:
            image_features = batch['image_features']
            expert_labels = batch['expert_labels']
            ml_predictions = batch['ml_predictions']
            true_labels = batch['true_labels']
            image_ids = batch['Image_ID']

            outputs = model(image_features, expert_labels, ml_predictions)
            all_outputs.append(outputs)
            all_labels.append(true_labels)
            all_image_ids.extend(image_ids)
            all_expert_labels.append(expert_labels)
            all_ml_predictions.append(ml_predictions)
            all_gating_weights.append(model.gating(image_features, expert_labels, ml_predictions))  # è·å–æƒé‡

    all_outputs = torch.cat(all_outputs).numpy()
    all_labels = torch.cat(all_labels).numpy()
    all_expert_labels = torch.cat(all_expert_labels).numpy()
    all_ml_predictions = torch.cat(all_ml_predictions).numpy()
    all_gating_weights = torch.cat(all_gating_weights).numpy()

    # åº”ç”¨é˜ˆå€¼
    predictions = (all_outputs >= np.array(thresholds)).astype(int)
    fusion_hamming_loss = hamming_loss(all_labels, predictions)
    expert_hamming_loss = hamming_loss(all_labels, (all_expert_labels >= 0.5).astype(int))
    ml_hamming_loss = hamming_loss(all_labels, (all_ml_predictions >= 0.5).astype(int))



    # print(f"èåˆåHamming Loss: {fusion_hamming_loss:.4f}")

    # è®¡ç®—Accuracy
    accuracy_fusion = (predictions == all_labels).mean()
    print(f"èåˆåAccuracy: {accuracy_fusion:.4f}")

    map_fusion = calculate_map(all_outputs, all_labels)
    map_expert = calculate_map(all_expert_labels, all_labels)
    map_ml = calculate_map(all_ml_predictions, all_labels)


    # è®¡ç®—AUC
    auc_fusion = roc_auc_score(all_labels, all_outputs, average='macro')
    auc_expert = roc_auc_score(all_labels, all_expert_labels, average='macro')
    auc_ml = roc_auc_score(all_labels, all_ml_predictions, average='macro')

    # è¾“å‡ºæƒé‡
    # print(f"Gating Weights: {all_gating_weights}")

    f1_fusion = f1_score(all_labels, predictions, average='macro')
    f1_expert = f1_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    f1_ml = f1_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')

    precision_fusion = precision_score(all_labels, predictions, average='macro')
    precision_expert = precision_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    precision_ml = precision_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')

    recall_fusion = recall_score(all_labels, predictions, average='macro')
    recall_expert = recall_score(all_labels, (all_expert_labels >= 0.5).astype(int), average='macro')
    recall_ml = recall_score(all_labels, (all_ml_predictions >= 0.5).astype(int), average='macro')

    # è®¡ç®—é€ç±»æŒ‡æ ‡
    precision_per_class = precision_score(all_labels, predictions, average=None)
    recall_per_class = recall_score(all_labels, predictions, average=None)
    
    # è®¡ç®—é€ç±»Accuracyï¼ˆæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼‰
    accuracy_per_class = [(predictions[:, i] == all_labels[:, i]).mean() for i in range(4)]
    # è®¡ç®—é€ç±»F1åˆ†æ•°
    f1_per_class = f1_score(all_labels, predictions, average=None)
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„AUC
    auc_fusion_per_label = []
    for i, label in enumerate(['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass']):
        try:
            auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])
            auc_fusion_per_label.append(auc)
            print(f"{label} AUC: {auc:.4f}")
        except ValueError:
            auc_fusion_per_label.append(np.nan)
            print(f"{label} AUC: æ— æ³•è®¡ç®—ï¼ˆå¯èƒ½æ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»åˆ«ï¼‰")
    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„MAP
    map_fusion_per_label = []
    for i, label in enumerate(['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass']):
        map_value = average_precision_score(all_labels[:, i], all_outputs[:, i])
        map_fusion_per_label.append(map_value)


    # è¾“å‡ºé€ç±»æŒ‡æ ‡
    # print("\né€ç±»æŒ‡æ ‡:")
    # for i, label in enumerate(['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass']):
    #     print(f"{label}:")
    #     print(f"  Accuracy: {accuracy_per_class[i]:.4f}")
    #     print(f"  Precision: {precision_per_class[i]:.4f}")
    #     print(f"  Recall: {recall_per_class[i]:.4f}")
    #     print(f"  F1 Score: {f1_per_class[i]:.4f}")
    #     print(f"  AUC: {auc_fusion_per_label[i]:.4f}")
    #     print(f"  MAP: {map_fusion_per_label[i]:.4f}")

    print(f"èåˆç»“æœ Hamming_loss:{fusion_hamming_loss:.4f}, MAP:{map_fusion:.4f}, AUC: {auc_fusion:.4f}, F1: {f1_fusion:.4f}, Precision: {precision_fusion:.4f}, Recall: {recall_fusion:.4f}")
    # print(f"äººç±»ä¸“å®¶ Hamming_loss:{expert_hamming_loss:.4f}, MAP:{map_expert:.4f}, AUC: {auc_expert:.4f}, F1: {f1_expert:.4f}, Precision: {precision_expert:.4f}, Recall: {recall_expert:.4f}")
    # print(f"æœºå™¨ä¸“å®¶ Hamming_loss:{ml_hamming_loss:.4f}, MAP:{map_ml:.4f}, AUC: {auc_ml:.4f}, F1: {f1_ml:.4f}, Precision: {precision_ml:.4f}, Recall: {recall_ml:.4f}")

    # print(f"Gating Weights: {all_gating_weights}")
    

    results_df = pd.DataFrame(all_outputs, columns=['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass'])
    results_df['Image_ID'] = all_image_ids
    results_df.to_csv(output_csv, index=False)
    print(f"ç»“æœå·²ä¿å­˜åˆ° {output_csv}")

    log_file='evaluation_Stacking.txt'
    with open(log_file, 'a') as f:
        # f.write(f"èåˆåAccuracy: {accuracy_fusion:.4f}\n")
        f.write(f"æ•´ä½“æŒ‡æ ‡:\n")
        f.write(f"Hamming Loss: {fusion_hamming_loss:.4f}, Accuracy: {accuracy_fusion:.4f}\n")
        f.write(f"MAP:{map_fusion:.4f}, AUC: {auc_fusion:.4f}, F1: {f1_fusion:.4f}, Precision: {precision_fusion:.4f}, Recall: {recall_fusion:.4f}\n")
        # f.write(f"äººç±»ä¸“å®¶ MAP:{map_expert:.4f}, AUC: {auc_expert:.4f}, F1: {f1_expert:.4f}, Precision: {precision_expert:.4f}, Recall: {recall_expert:.4f}\n")
        # f.write(f"æœºå™¨ä¸“å®¶ MAP:{map_ml:.4f}, AUC: {auc_ml:.4f}, F1: {f1_ml:.4f}, Precision: {precision_ml:.4f}, Recall: {recall_ml:.4f}\n")
        f.write(f"é€ç±»æŒ‡æ ‡:\n")
        for i, label in enumerate(['Fracture', 'Pneumothorax', 'Airspace_Opacity', 'Nodule_Or_Mass']):
            f.write(f"{label}: Accuracy: {accuracy_per_class[i]:.4f}, AUC: {auc_fusion_per_label[i]:.4f}, MAP: {map_fusion_per_label[i]:.4f}, Precision: {precision_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, F1 Score: {f1_per_class[i]:.4f}\n")
        f.write(f"\n")

# å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

confusion_matrix = np.array([
    [0.42, 0.01, 0.23, 0.02],
    [0.01, 0.71, 0.19, 0.02],
    [0.00, 0.01, 0.83, 0.03],
    [0.01, 0.01, 0.17, 0.62]
])



# è®¾ç½®è·¯å¾„
image_folder = '/data/user24262904/My/ChestX-ray14/images'
data_path = "all_predictions.csv"

data = pd.read_csv(data_path)

# æå–å¹¶ä¿å­˜ç‰¹å¾
feature_file = 'image_features.npy'
if not os.path.exists(feature_file):
    extract_and_save_features(data, image_folder, transform, feature_file)


# æ•°æ®åˆ†å‰²
train_data = data[data['Set_ID_x'] == 'train']
val_data = data[data['Set_ID_x'] == 'val']
test_data = data[data['Set_ID_x'] == 'test']

train_dataset = FusionDataset(train_data, feature_file)
val_dataset = FusionDataset(val_data, feature_file)
test_dataset = FusionDataset(test_data, feature_file)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# åˆå§‹åŒ–æ¨¡å‹
# åˆå§‹åŒ–MoEæ¨¡å‹
print("è®­ç»ƒç¬¬1æ¬¡-----------------------------------")
# è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
model1 = MoE(input_dim=2048, num_experts=2, num_labels=4, confusion_matrix=confusion_matrix)# è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# è®­ç»ƒå’Œæµ‹è¯•
thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results1.csv')

# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬2æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results2.csv')

# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬3æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4,)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results3.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬4æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results4.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬5æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results5.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬6æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results6.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬7æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results7.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬8æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results8.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬9æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results9.csv')


# # æ•°æ®åˆ†å‰²
# train_data = data[data['Set_ID_x'] == 'train']
# val_data = data[data['Set_ID_x'] == 'val']
# test_data = data[data['Set_ID_x'] == 'test']

# train_dataset = FusionDataset(train_data, feature_file)
# val_dataset = FusionDataset(val_data, feature_file)
# test_dataset = FusionDataset(test_data, feature_file)

# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        
# # åˆå§‹åŒ–æ¨¡å‹
# # åˆå§‹åŒ–MoEæ¨¡å‹
# print("è®­ç»ƒç¬¬10æ¬¡-----------------------------------")
# # è¿è¡Œè¯¥å‡½æ•°ï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç›¸åŒçš„åˆå§‹æ¡ä»¶
# model1 = MoE(input_dim=2048, num_experts=2, num_labels=4)  # è¾“å…¥ç»´åº¦ä¸ºå›¾åƒç‰¹å¾çš„ç»´åº¦ï¼Œæ ‡ç­¾æ•°é‡ä¸º4
# criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±
# optimizer1 = optim.Adam(model1.parameters(), lr=0.00001)  # è°ƒä½å­¦ä¹ ç‡,å¢å¤§æ¢¯åº¦å°±å˜ä¸º0äº†
# # è®­ç»ƒå’Œæµ‹è¯•
# thresholds1 = train_model(model1, train_loader, val_loader, criterion, optimizer1, epochs=30, model_save_path='best_model1.pth')
# evaluate_model(model1, test_loader, thresholds=thresholds1, model_path='best_model1.pth', output_csv='Stacking_test_results10.csv')
